\documentclass{article}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}

\newcommand{\real}{\mathbb{R}}
\usepackage{verbatim}

\begin{document}
\setcounter{section}{4}
\section{Basic machine learning}


Machine Learning (ML) or Statistical Learning (SL) refers to the
techniques ued to model, describe and understand complex data
sets. This emcompasses basic methods from linear regressions to more
complex classifying techniques.

The kind of questions that machine learning wants to address are the
following:

\begin{itemize}
\item{Can we predict how a viewer would rate a movie? \footnote{See: \texttt{http://www.youtube.com/watch?v=mbyG85GZ0PI}}}
\item{Can we predict a possible rise of criminality in a
  city?\footnote{See: \texttt{https://github.com/finiterank/homicidios}}}
\item{Can we guess whether a certain dot in a picture is a distant
  galaxy or a star?}
\item{What are the common traits between different genome sequences?}
\end{itemize}

What do these questions have in common? First of all, we believe that
a pattern exists that can lead us to the answer. Secondly, we cannot
make a mathematical model that describe the pattern, i.e. we cannot
build a model from basic physics that describe how a viewer would rate
a movie. The third \emph{very important} commonality is that we have
data on it; we have a measurement or series of measurements that can
help us to discover the pattern.

With these three components you are ready to apply machine learning
techniques. 


In this chapter we talk about three different machine learning
techniques: linear regresion, principal component analysis (PCA) and k-means
clustering. Before making a good description of each one of these
techniques we will make a short linear algebra review, which is needed
for the linear regresion discussion.

The last two chapters on PCA and k-means clustering can be studied in
great detail from Chapter 10 in the book "An Introduction to
Statistical Learning, with applications in R"  (Springer, 2013). You
can get a pdf from 

\verb"http://www-bcf.usc.edu/~gareth/ISL/data.html".



\subsection{Linear Algebra}
\subsubsection{Matrices}
We will consider a matrix as a rectangular array of real (or complex) numbers arranged in sets of $m$ rows with $n$ entries. The set of matrices with real coefficients with size $m\times n$ is called $\real^{m\times n}$. In the context of this chapter vectors will be understood as a column vectors, that is as a matrix of one column belonging to the set $\real^{m}$. 

We will refer to the $a_{ij}$ element of matrix ${\mathbf A}$ to indicate the element located in row $i$ and column $j$. In the case of a vector ${\mathbf x}$ we will refer to its components as $x_{i}$.


\begin{displaymath}
\mathbf{A} = 
\left[
\begin{array}{cccc}
a_{11}  & a_{12}  & \dots & a_{1n}   \\
a_{21}  &  \ddots &  & \vdots  \\
\vdots  &  & \cdots & a_{mn}   
\end{array}
\right]
\qquad
\mathbf{x} = 
\left[
\begin{array}{c}
x_{1}     \\
x_2          \\
\vdots     \\
x_{m}   
\end{array}
\right]
\end{displaymath}


\subsubsection{Matrices as pointers in C}
Matrices in C are best stored as one dimensional arrays. This implies that there is convention to translate the position $i,j$ in the matrix into the index $p$ in the one dimensional array. 

This convention in C is called {\bf raw major order}, where the second index moves continuously in memory. It means that the following $3\times 3$ matrix:


\begin{displaymath}
\mathbf{A} = 
\left[
\begin{array}{ccc}
a_{11}  & a_{12} & a_{13}   \\
a_{21}  & a_{22}   & a_{23}  \\
a_{31}  & a_{32} & a_{33}   
\end{array}
\right]
\end{displaymath}

will be stored in memory as the following one dimensional array

\begin{displaymath}
\mathbf{a} = 
\left[
\begin{array}{ccccccccc}
a_{11}  & a_{12} & a_{13}   & a_{21}  & a_{22}   & a_{23}  & a_{31}  & a_{32} & a_{33}   
\end{array}
\right]
\end{displaymath}.

In FORTRAN the convention is the opposite, the first index moves continously in memory. This convention is known as {\bf column major order}. 

In the following example we define a matrix $5\times 5$ and initialize it to be 
the identity.

\verbatiminput{../hands_on/lin_algebra/identity.c}


\subsubsection{Functions taking pointers in C}
We go into the details of dealing with a array/matrix operation in C. Most of the operations we make onto a matrix will be done using functions. Functions in C work with variables passed as reference, not as values. That means that if you want to have a variable modified, you have to give the address where the variable lives in memory. In practice, that means that you should work only with pointers.


In the next example we define a matrix $3\times3$ an take its transpose.

\verbatiminput{../hands_on/lin_algebra/transpose.c}

What would happen if you try to modify the variable \verb"n_x" inside a function?

\subsubsection{GNU Scientific Library}

Most of the computations that we will perform in C will be done using
external libraries. We will tap on the work done by other
people. However simple it may seem it is important that you understand
how external libraries can be used in C.  

\begin{verbatim}
sudo apt-get install libgsl0-dev
sudo apt-get install liblapack-dev
\end{verbatim}

Let's start by considering the following example that uses the gsl library to compute the the Bessel function $J_{0}(x)$ for $x=5$:

\begin{verbatim}
#include <stdio.h>
#include <gsl/gsl_sf_bessel.h>
     
int main (void){
       double x = 5.0;
       double y = gsl_sf_bessel_J0 (x);
       printf ("J0(%g) = %.18e\n", x, y);
       return 0;
}
\end{verbatim}

If the setup for the library are the standard ones, the program can be compiled as

\begin{verbatim}
cc -lgsl -lgslcblas -lm test.c 
\end{verbatim}



If you are under UBUNTU probably the compilation order matters and you should instead write

\begin{verbatim}
cc test.c -lgsl -lgslcblas -lm 
\end{verbatim}

Executing the program will produce the following results:

\begin{verbatim}
J0(5) = -1.775967713143382642e-01
\end{verbatim}.


GSL also has special libraries to deal with linear algebra operations. A detailed webpage including the manual and useful examples can be found here:

\begin{verbatim}
http://www.gnu.org/software/gsl/manual/html_node/
\end{verbatim}

Now try to follow the structure in the following program that takes a square matrix and computes its eigenvalues\footnote{Example taken from the example manual of the GSL webpage.}):


\verbatiminput{../hands_on/lin_algebra/eigen.c}

\subsubsection{Linear Algebra and Regressions: Mathematical Aspects}

We are now interested in giving a mathematical background to the problem of regressions in physics\footnote{This section is based on German Prieto notes on inversion theory in geosciences.}. 
The following situation is very common in physics, we the measurement of some quantity, say the spectra from a distant galaxy and we want infer other physical properties in that galaxy (mass and age, for instance).

In general the methods to infer the properties of a model from the measurements is known as inverse theory. The methods that allows us to pick the most probable model from a series of measurements will be called statistical inference. Both are related and have many similarities.

In general mathematical terms, if we have a model for a physical phenomenon described by a set of parameter and we have a theory that can relate those parameters to produce an observable, we can write

\begin{equation}
d_{i} = G_i (\mathbf{m}),
\end{equation}

where ${\mathbf m}$ contains all the instances of the physical parameters in my model (i.e time, masses, etc), $d$ is the observed data and $G$ is the is the theory that produces the observed data $d$ from the parameters in the model $m$. The index $i$ goes over the number of measurements that you have.

Why is this problem hard? Because you might have a model described by infinite number of parameters and a finite number of measurements. In that case we talk about and undetermined problem. You might decide to oversimplify the problem and reduce the number of parameters, if you have more measurements than parameters in the model, you are in the extreme of an overdetermined problem.

This problem can be written as matrix multiplications:


\begin{equation}
\mathbf{d} = \mathbf{G}\mathbf{m}
\end{equation}
where $\mathbf{d}$ represents the vector with the data. $\mathbf{G}$ is the theory that we use (electrodynamics, gravitation, etc) for predicting data and $\mathbf{m}$ is the model that we wish to obtain.


\subsubsection{Linear regressions}

The measurements that you have might be the positions as a function of time of ball flying through the air. You decided that the physics that best apply in that case are the kinematics of a movement with uniform acceleration. Under that theory the model that describes the position as a function of time is the following:

\begin{equation}
y(t) = m_1 + m_2 t + (1/2) m_3 t^2
\end{equation}

where $y(t)$ represents the altitude of the object at time $t$, and the three $(N=3$) model parameters $m_i$ are associated with a constant, slope and quadratic terms. Note that even if the function is quadratic, the problem in question is linear for the three parameters. 

If we have $M$ discrete measurements $y_i$ at times $t_i$, the linear regression problem or inverse problem can be written in the form
\begin{equation}
\left[
\begin{array}{c}
  y_1   \\
  y_2   \\
  \vdots \\
  y_M
\end{array}
\right] = 
\left[
\begin{array}{ccc}
  1 & t_1 & {\scriptstyle{\frac{1}{2}}}t_1^2 \\
  1  &t_2 &  {\scriptstyle{\frac{1}{2}}}t_2^2\\
  \vdots & \vdots  & \vdots\\
  1 & t_M &  {\scriptstyle{\frac{1}{2}}}t_M^2
\end{array}
\right]
\left[
\begin{array}{c}
  m_1   \\
  m_2    \\
  m_3
\end{array}
\right] 
\end{equation}
When the regression model is linear in the unknown parameters, then we call this a linear regression or linear inverse problem. 

Solving this problem means finding a model $\mathbf{\hat{m}}$ that best reproduced the observed values. \emph{Best reproducing} means seeing how different are the observations $\mathbf{d}$ when compared to the results of the theory computed over the model: $\mathbf{G} \mathbf{\hat{m}}$.

Suppose we are given a collection of $M$ measurements of a property to
form a vector $\mathbf{d} \in \real^M$. We know forward problem such
that we can predict the data from a known model $\mathbf{m} \in
\real^N$. That is, we know the $N$ vectors $\mathbf{g}_k \in \real^M$
such that   

\begin{equation}
\mathbf{d} = \sum_{k=1}^{N} \mathbf{g}_k m_k = \mathbf{G} \mathbf{m}
\end{equation}
where
\begin{equation}
\mathbf{G} = \left[ \mathbf{g}_1, \mathbf{g}_2, \dots, \mathbf{g}_N \right] 
\end{equation}

We are looking for a model $\hat{\mathbf{m}}$ that minimizes the size of the residual vector defined

\begin{equation}
\mathbf{r} = \mathbf{d} -  \sum_{k=1}^{N} \mathbf{g}_k \hat{m}_k
\end{equation}

We do not expect to have an exact fit so there will be some error, and we use a norm to measure the size of the residual
\begin{equation}
\| \mathbf{r} \| = \| \mathbf{d} - \mathbf{Gm} \|
\end{equation}

For the least squares problem we use the $L2$ or Euclidean norm 
\begin{equation}
\| \mathbf{r} \|_2 = \sqrt{\sum_{k=1}^M r^2_k}
\end{equation}

Minimizing this quantity will give you the best parameters of your model.


We won't derive the results here, but assuming the inverse of $(\mathbf{G}^T \mathbf{G})$ exists, we can isolate $\hat{\mathbf{m}}$ to end up with

\begin{equation}
\hat{\mathbf{m}} = {(\mathbf{G}^T \mathbf{G})}^{-1}\mathbf{G}^T \mathbf{d}   
\end{equation}

Note that the matrix ${(\mathbf{G}^T \mathbf{G})}$ is a square $N\times N$ matrix and $\mathbf{G}^T\mathbf{d}$ is an $N$ column vector. 

\subsection{Principal Component Analysis}

Principal Component Analysis is used when you have a set of
measurements or individuals described by a large number of variables
and you want (hope to be able) to reduce that dimensionality.

This time we start from $M$ different measurements of points in $N$
dimensions. The dimensions might be different physical quantities
(velocities, masses), or might be the points in wavelength where you
measure the intensity of a spectrum. 

We start by measuring the covariance matrix, ${\bf\Sigma}$ of
different dimension of all the data points. This covariance matrix
will be a $N\times N$ matrix where each element is computed as: 

\begin{equation}
\sigma_{i,j} = \frac{\sum_{k=1}^{M} (d^{k}_i - \bar{d_i})(d^{k}_j -\bar{d_{j}})}{M-1}
\end{equation}

In this notation there are $M$ vector of measurements ${\mathbf d}^{1}$,\ldots,${\mathbf d}^{M}$. Where each vector has $N$ components, for instance the $k$-th measurement will be ${\mathbf d}^{k}=(d^k_{1},\ldots,d^k_{N})$.

Some properties of the covariance matrix are the following:
\begin{enumerate}
\item The matrix ${\bf\Sigma}$ is symmetric;
  $\sigma_{ij}=\sigma_{ji}$.
\item Diagonal elements correspond to the the variance of the
  corresponding variables $\sigma_{ii}=\sigma_i^2$.
\item The covariance matrix is defined as non-negative
  ($\sigma_{ij}\geq 0$).
\end{enumerate}

The value of each entry in the covariance matrix depends on the units
defined to make the measurements. An alternative is to use the
correlation matrix,${\mathbf R}$, which normalizes each measuremente
by the square root of the variance along that dimension. As a result
the diagonal elements of ${\mathbf R}$ are unity $\rho_{ii}=1$.

The PCA technique consists in calculating the covariance matrix (or
the correlation matrix to avoid the arbitrary unit scaling) and
calculating its eigenvalues and eigenvectors. If you rank the
eigenvalues in descending order the associated eigenvectors will tell
the directions (in $N$ dimensional space) where the variance
decreases. With this you will be able to reduce the dimensionality of
your problem and express each point as the sum of the eigenvectors of
this matrix.

As a more concrete example of the application of Principal Component
Analysis let's take some data from the context of neurosciences. We
have a patient and we are measuring her brain activity through
encephalograms taken with 24 electrodes at different points on its
head. The encephalogram measures electric activity as a function of
time. In this case we have now 24 different signals taken at 400
different times. In the figure we show two different signals.


\subsection{K-means clustering}

Clustering means to the ways of finding groups in a set of data.  This
means that within a set of observations there will be groups that are
similar and others are different. A clear application can be though in
a group of students that is measured by two numbers: the amount of
hours spent studyng every week and the final grade in a course. In
this case there might be four clusters; students that

\begin{enumerate}
\item{Spend a lot of time studying and have good grades}
\item{Don't spend time studying and have good grades}
\item{Spend a lot of time studying and have bad grades}
\item{Don't spend time studying and have bad grades}
\end{enumerate}

The question is. How can we design an algorithm to find these
clusters?

$K$-means clustering is an easy way to partition a data set into $K$
clusters. We denote by $S_1$, $\cdots$, $C_K$ the $K$ sets containing
all the observations. These sets satisfy two properties: (1) the sum
of all the sets is equal to the original observation set and (2) the
clusters are disjoint. 

A possible way to define the partitioning that splits the observations
into $K$ clusters is to minimize the sum of the variance of each
cluster.

\begin{displaymath}
\sum_{k=1}^K \frac{1}{|C_k|}\sum_{m,m^\prime \in C_k}\sum_{n=1}^{N}
(d^{m}_{n} - d^{m^\prime}_{n})^2
\end{displaymath}
%
meaning that the variance is the average of all the euclidian interparticle
distances between all the pairs in the cluster.


An heuristic algorithm to minimize this variance is the following

\begin{itemize}
\item Initialize by asigning a number from $1$ to $K$ to each
  observation. 
\item Iterate until the number assigned to each observation stops
  changing.
\begin{itemize}
\item For each of set of points wih the label $k$, compute the
  centroid.
\item Assign each measurement to the cluster number $k$ whose centroid
  is closest.
\end{itemize}
\end{itemize}

\end{document}
