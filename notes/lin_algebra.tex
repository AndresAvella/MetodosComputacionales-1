\documentclass{article}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}

\newcommand{\real}{\mathbb{R}}
\usepackage{verbatim}

\begin{document}
\setcounter{section}{3}
\section{Linear Algebra and Regressions: Technical Aspects in C}
In this chapter we will see how basic linear algebra concepts will serve us to create models to interpret a set of measured points.

With basic linear algebra and matrix notation, we will be able to estimate parameters in a model (i.e. by making least square fitting) or explore data by using non parametric descriptions like Principal Component Analysis.

Before diving into direct applications we will start by 
understating what is the best way to use matrices in C and how to take advantage of the GNU Scientific Library.


\subsection{Matrices}
We will consider a matrix as a rectangular array of real (or complex) numbers arranged in sets of $m$ rows with $n$ entries. The set of matrices with real coefficients with size $m\times n$ is called $\real^{m\times n}$. In the context of this chapter vectors will be understood as a column vectors, that is as a matrix of one column belonging to the set $\real^{m}$. 

We will refer to the $a_{ij}$ element of matrix ${\mathbf A}$ to indicate the element located in row $i$ and column $j$. In the case of a vector ${\mathbf x}$ we will refer to its components as $x_{i}$.


\begin{displaymath}
\mathbf{A} = 
\left[
\begin{array}{cccc}
a_{11}  & a_{12}  & \dots & a_{1n}   \\
a_{21}  &  \ddots &  & \vdots  \\
\vdots  &  & \cdots & a_{mn}   
\end{array}
\right]
\qquad
\mathbf{x} = 
\left[
\begin{array}{c}
x_{1}     \\
x_2          \\
\vdots     \\
x_{m}   
\end{array}
\right]
\end{displaymath}


\subsection{Matrices as pointers in C}
Matrices in C are best stored as one dimensional arrays. This implies that there is convention to translate the position $i,j$ in the matrix into the index $p$ in the one dimensional array. 

This convention in C is called {\bf raw major order}, where the second index moves continuously in memory. It means that the following $3\times 3$ matrix:


\begin{displaymath}
\mathbf{A} = 
\left[
\begin{array}{ccc}
a_{11}  & a_{12} & a_{13}   \\
a_{21}  & a_{22}   & a_{23}  \\
a_{31}  & a_{32} & a_{33}   
\end{array}
\right]
\end{displaymath}

will be stored in memory as the following one dimensional array

\begin{displaymath}
\mathbf{a} = 
\left[
\begin{array}{ccccccccc}
a_{11}  & a_{12} & a_{13}   & a_{21}  & a_{22}   & a_{23}  & a_{31}  & a_{32} & a_{33}   
\end{array}
\right]
\end{displaymath}.

In FORTRAN the convention is the opposite, the first index moves continously in memory. This convention is known as {\bf column major order}. 

In the following example we define a matrix $5\times 5$ and initialize it to be 
the identity.

\verbatiminput{../hands_on/lin_algebra/identity.c}


\subsection{Functions taking pointers in C}
We go into the details of dealing with a array/matrix operation in C. Most of the operations we make onto a matrix will be done using functions. Functions in C work with variables passed as reference, not as values. That means that if you want to have a variable modified, you have to give the address where the variable lives in memory. In practice, that means that you should work only with pointers.


In the next example we define a matrix $3\times3$ an take its transpose.

\verbatiminput{../hands_on/lin_algebra/transpose.c}

What would happen if you try to modify the variable \verb"n_x" inside a function?

\subsection{GNU Scientific Library}

Most of the computations that we will perform in C will be done using external libraries. We will tap on the work done by other people. However simple it may seem it is important that you understand how external libraries can be used in C.

Let's start by considering the following example that uses the gsl library to compute the the Bessel function $J_{0}(x)$ for $x=5$:

\begin{verbatim}
#include <stdio.h>
#include <gsl/gsl_sf_bessel.h>
     
int main (void){
       double x = 5.0;
       double y = gsl_sf_bessel_J0 (x);
       printf ("J0(%g) = %.18e\n", x, y);
       return 0;
}
\end{verbatim}

If the setup for the library are the standard ones, the program can be compiled as

\begin{verbatim}
cc -lgsl -lm test.c 
\end{verbatim}



If you are under UBUNTU probably the compilation order matters and you should instead write

\begin{verbatim}
cc test.c -lgsl -lm 
\end{verbatim}

Executing the program will produce the following results:

\begin{verbatim}
J0(5) = -1.775967713143382642e-01
\end{verbatim}.


GSL also has special libraries to deal with linear algebra operations. A detailed webpage including the manual and useful examples can be found here:

\begin{verbatim}
http://www.gnu.org/software/gsl/manual/html_node/
\end{verbatim}

Now try to follow the structure in the following program that takes a square matrix and computes its eigenvalues\footnote{Example taken from the example manual of the GSL webpage.}):


\verbatiminput{../hands_on/lin_algebra/eigen.c}

\section{Linear Algebra and Regressions: Mathematical Aspects}

We are now interested in giving a mathematical background to the problem of regressions in physics\footnote{This section is based on German Prieto notes on inversion theory in geosciences.}. 
The following situation is very common in physics, we the measurement of some quantity, say the spectra from a distant galaxy and we want infer other physical properties in that galaxy (mass and age, for instance).

In general the methods to infer the properties of a model from the measurements is known as inverse theory. The methods that allows us to pick the most probable model from a series of measurements will be called statistical inference. Both are related and have many similarities.

In general mathematical terms, if we have a model for a physical phenomenon described by a set of parameter and we have a theory that can relate those parameters to produce and observable, we can write

\begin{equation}
d_{i} = G (\mathbf{m}_{i}),
\end{equation}

where ${\mathbf m}$ contain-ts all the instances of the physical parameters in my model (i.e time, masses, etc), $d$ is the observed data and $G$ is the is the theory that produces the observed data $d$ from the parameters in the model $m$. The index $i$ goes over the number of measurements that you have.

Why is this problem hard? Because you might have a model described by infinite number of parameters and a finite number of measurements. In that case we talk about and undetermined problem. You might decide to oversimplify the problem and reduce the number of parameters, if you have more measurements than parameters in the model, you are in the extreme of an overdetermined problem.

This problem can be written as matrix multiplications:


\begin{equation}
\mathbf{d} = \mathbf{G}\mathbf{m}
\end{equation}
where $\mathbf{d}$ represents the vector with the data. $\mathbf{G}$ is the theory that we use (electrodynamics, gravitation, etc) for predicting data and $\mathbf{m}$ is the model that we wish to obtain.


\subsection{Getting Practical: linear regressions}

The measurements that you have might be the positions as a function of time of ball flying through the air. You decided that the physics that best apply in that case are the kinematics of a movement with uniform acceleration. Under that theory the model that describes the position as a function of time is the following:

\begin{equation}
y(t) = m_1 + m_2 t + (1/2) m_3 t^2
\end{equation}

where $y(t)$ represents the altitude of the object at time $t$, and the three $(N=3$) model parameters $m_i$ are associated with a constant, slope and quadratic terms. Note that even if the function is quadratic, the problem in question is linear for the three parameters. 

If we have $M$ discrete measurements $y_i$ at times $t_i$, the linear regression problem or inverse problem can be written in the form
\begin{equation}
\left[
\begin{array}{c}
  y_1   \\
  y_2   \\
  \vdots \\
  y_M
\end{array}
\right] = 
\left[
\begin{array}{ccc}
  1 & t_1 & {\scriptstyle{\frac{1}{2}}}t_1^2 \\
  1  &t_2 &  {\scriptstyle{\frac{1}{2}}}t_2^2\\
  \vdots & \vdots  & \vdots\\
  1 & t_M &  {\scriptstyle{\frac{1}{2}}}t_M^2
\end{array}
\right]
\left[
\begin{array}{c}
  m_1   \\
  m_2    \\
  m_3
\end{array}
\right] 
\end{equation}
When the regression model is linear in the unknown parameters, then we call this a linear regression or linear inverse problem. 

Solving this problem means finding a model $\mathbf{\hat{m}}$ that best reproduced the observed values. \emph{Best reproducing} means seeing how different are the observations $\mathbf{d}$ when compared to the results of the theory computed over the model: $\mathbf{G} \mathbf{\hat{m}}$.

Suppose we are given a collection of $M$ measurements of a property to form a vector $\mathbf{d} \in \real^M$. From our geophysics we know the forward problem such that we can predict the data from a known model $\mathbf{m} \in \real^N$. That is, we know the $N$ vectors $\mathbf{g}_k \in \real^M$ such that 
\begin{equation}
\mathbf{d} = \sum_{k=1}^{N} \mathbf{t}_k m_k = \mathbf{G} \mathbf{m}
\end{equation}
where
\begin{equation}
\mathbf{G} = \left[ \mathbf{g}_1, \mathbf{g}_2, \dots, \mathbf{g}_N \right] 
\end{equation}

We are looking for a model $\hat{\mathbf{m}}$ that minimizes the size of the residual vector defined

\begin{equation}
\mathbf{r} = \mathbf{d} -  \sum_{k=1}^{N} \mathbf{t}_k \hat{m}_k
\end{equation}

We do not expect to have an exact fit so there will be some error, and we use a norm to measure the size of the residual
\begin{equation}
\| \mathbf{r} \| = \| \mathbf{d} - \mathbf{Gm} \|
\end{equation}

For the least squares problem we use the $L2$ or Euclidean norm 
\begin{equation}
\| \mathbf{r} \|_2 = \sqrt{\sum_{k=1}^M r^2_k}
\end{equation}

Minimizing this quantity will give you the best parameters of your model.


We won't derive the results here, but assuming the inverse of $(\mathbf{G}^T \mathbf{G})$ exists, we can isolate $\hat{\mathbf{m}}$ to end up with

\begin{equation}
\hat{\mathbf{m}} = {(\mathbf{G}^T \mathbf{G})}^{-1}\mathbf{G}^T \mathbf{d}   
\end{equation}

Note that the matrix ${(\mathbf{G}^T \mathbf{G})}$ is a square $N\times N$ matrix and $\mathbf{G}^T\mathbf{d}$ is an $N$ column vector. 

\subsection{Getting Practical: Principal Component Analysis}

Many times in physics you don't have a theory, not event a model, of the data your are getting. However, in the process of building a model you would like to explore your measurements and get a handle on the physical process that might affect your data.

One of the first techniques that you have to learn in statistical analysis is Principal Component Analysis (PCA). The objective of this section is that you learn to perform and interpret data using this technique.

This time we start from $M$ different measurements of points in $N$ dimensions. The dimensions might be different physical quantities (velocities, masses), or might be the points in wavelength where you measure the intensity of a spectrum.

We start by measuring the covariance matrix of different dimension of all the data points. This covariance matrix will be a $N\times N$ matrix where each element is computed as:

\begin{equation}
\sigma_{i,j} = \frac{\sum_{k=1}^{M} (d^{k}_i - \bar{d_i})(d^{k}_j -\bar{d_{j}})}{M-1}
\end{equation}

In this notation there are $M$ vector of measurements ${\mathbf d}^{1}$,\ldots,${\mathbf d}^{M}$. Where each vector has $N$ components, for instance the $k$-th measurement will be ${\mathbf d}^{k}=(d^k_{1},\ldots,d^k_{N})$.

The PCA technique consists in calculating the covariance matrix and calculating its eigenvalues and eigenvectors. If you rank the eigenvalues in descending order the associated eigenvectors will tell the directions (in $N$ dimensional space) where the variance decreases. With this you will be able to reduce the dimensionality of your problem and express each point as the sum of the eigenvectors.

\end{document}
